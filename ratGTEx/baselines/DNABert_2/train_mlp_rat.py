#!/usr/bin/env python3
"""
Train MLP Classifier on DNABERT-2 Concatenated Embeddings for Rat Data
"""
import numpy as np
import pandas as pd
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, confusion_matrix
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import argparse
import json
import logging

logging.basicConfig(level=logging.INFO, format='%(message)s')

def main(args):
    data = np.load(args.input_file, allow_pickle=True)
    variant_ids = data['variant_ids']
    embeddings = data['embeddings']
    labels = data['labels']
    
    print(f"Loaded: {len(variant_ids)} variants, {embeddings.shape}")
    print(f"Labels: {np.bincount(labels)}")
    
    X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(
        embeddings, labels, variant_ids,
        test_size=0.2, random_state=42, stratify=labels
    )
    print(f"Split: train={len(X_train)}, test={len(X_test)}")
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print("Training MLP [256, 128]...")
    model = MLPClassifier(
        hidden_layer_sizes=(256, 128),
        activation='relu',
        solver='adam',
        max_iter=1000,
        random_state=42,
        early_stopping=True,
        n_iter_no_change=20,
        verbose=True,
        learning_rate_init=0.001,
        alpha=0.0001,
        batch_size=64
    )
    
    model.fit(X_train_scaled, y_train)
    print(f"Converged: {model.n_iter_} iterations, val_score={model.best_validation_score_:.4f}")
    
    y_pred = model.predict(X_test_scaled)
    y_proba = model.predict_proba(X_test_scaled)[:, 1]
    
    auroc = roc_auc_score(y_test, y_proba)
    auprc = average_precision_score(y_test, y_proba)
    accuracy = accuracy_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)
    
    print(f"\nTest Performance:")
    print(f"  AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}, Accuracy: {accuracy:.4f}")
    print(f"  Confusion Matrix:\n{cm}")
    
    joblib.dump(model, args.model_output_file)
    joblib.dump(scaler, args.model_output_file.replace('.joblib', '_scaler.joblib'))
    
    metrics = {
        'auroc': float(auroc),
        'auprc': float(auprc),
        'accuracy': float(accuracy),
        'confusion_matrix': cm.tolist(),
        'n_train': int(len(X_train)),
        'n_test': int(len(X_test)),
        'n_iterations': int(model.n_iter_),
        'best_validation_score': float(model.best_validation_score_)
    }
    
    with open(args.metrics_output_file, 'w') as f:
        json.dump(metrics, f, indent=2)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'DNABERT-2 MLP Rat\nAUROC={auroc:.4f}, AUPRC={auprc:.4f}')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig(args.confusion_matrix_file, dpi=300, bbox_inches='tight')
    
    # Save test set predictions for evaluation
    results_df = pd.DataFrame({
        'variant_id': ids_test,
        'true_label': y_test,
        'predicted_label': y_pred,
        'predicted_proba': y_proba
    })
    results_df.to_csv(args.proba_output_file, sep='\t', index=False)
    logging.info(f"Saved test set predictions: {args.proba_output_file}")
    logging.info("Note: Full predictions will be generated by the shell script using the trained model.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train MLP on DNABERT-2 embeddings for rat data")
    parser.add_argument("--input_file", type=str, required=True, help="Input NPZ file with embeddings")
    parser.add_argument("--model_output_file", type=str, required=True, help="Output joblib file for model")
    parser.add_argument("--confusion_matrix_file", type=str, required=True, help="Output PNG for confusion matrix")
    parser.add_argument("--metrics_output_file", type=str, required=True, help="Output JSON for metrics")
    parser.add_argument("--proba_output_file", type=str, required=True, help="Output TSV for variant scores")
    args = parser.parse_args()
    main(args)

